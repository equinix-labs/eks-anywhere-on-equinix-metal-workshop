{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-equinix-labs","title":"About Equinix Labs","text":"<p>Equinix Labs offers workshops, proof of concepts, and tools for exploring and bootstrapping Equinix digital infrastructure including Fabric, Metal, and Network Edge.</p>"},{"location":"#about-the-workshop","title":"About the workshop","text":"<p>In this workshop you will learn how to manually deploy and manage EKS-Anywhere in Equinix Metal</p> <p>The goals of this workshop are:</p> <ul> <li>Become familiar with the Equinix Metal and AWS EKS/EKS-A tools</li> <li>Installation options</li> <li>How to add new nodes to expand a cluster</li> <li>How to visualize your cluster in the Amazon EKS console</li> </ul>"},{"location":"#workshop-agenda","title":"Workshop agenda","text":"<p>This workshop is split into four parts:</p> Part Title Duration 1 Setup 10 minutes 2 Preparing Bare Metal for EKS Anywhere 15 minutes 3 EKS-A installation and configuration 15 minutes 4 Using EKS Connector 5 minutes 5 Conclusion 5 minutes"},{"location":"parts/conclusion/","title":"Conclusion","text":""},{"location":"parts/conclusion/#conclusion","title":"Conclusion","text":"<p>Thank you for participating in the workshop! Let's recap some of the key takeways that we've learned:</p> <ul> <li>How configure Equinix Metal environment to run EKS clusters</li> <li>Deploy Equinix Metal devices and configure advanced network settings</li> <li>Scale up cluster adding more worker nodes</li> </ul>"},{"location":"parts/conclusion/#next-steps","title":"Next Steps","text":"<ul> <li>Try out the Terraform Module to automate the entire setup process.</li> </ul>"},{"location":"parts/conclusion/#resources","title":"Resources","text":"<p>Here are a few other resources to look at to continue your Equinix Metal journey:</p> <ul> <li>Deploy @ Equinix: A one-stop shop for blogs, guides, and plenty of other resources.</li> <li>Equinix Metal Docs: Equinix Metal official documentation.</li> <li>Equinix Metal APIs: Programmatically interact with Equinix Metal</li> <li>Equinix Labs: Provides SDKs and Terraform modultes for Infrastructure as Code tools.</li> <li>Equinix Community: A global community for customers and Equinix users.</li> </ul>"},{"location":"parts/eks-connector/","title":"4. Using EKS Connector","text":""},{"location":"parts/eks-connector/#part-4-connect-the-cluster-to-eks-with-eks-connector","title":"Part 4: Connect the cluster to EKS with EKS Connector","text":"<p>This section covers the basic steps to connect your cluster to EKS with the EKS Connector. There are many more details (include pre-requisites like IAM permissions) in the EKS Connector Documentation.</p>"},{"location":"parts/eks-connector/#steps","title":"Steps","text":""},{"location":"parts/eks-connector/#1-setup-aws-credentials","title":"1. Setup AWS credentials","text":"<p>Follow the AWS documentation and set the environment variables with your authentication info for AWS in your eks-admin device. For example:</p> Bash<pre><code>export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE\nexport AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\nexport AWS_DEFAULT_REGION=us-west-2\n</code></pre>"},{"location":"parts/eks-connector/#2-register-the-cluster","title":"2. Register the cluster","text":"<p>Use <code>eksctl</code> to register the cluster</p> Bash<pre><code>CLUTER_NAME=\"My-Metal-Cluster\"\nAWS_REGION=\"eu-west-1\"\neksctl register cluster --name $CLUTER_NAME --provider other --region region-code\n</code></pre> <p>If it succeeded, the output will show several .yaml files that were created and need to be registered with the cluster. For example, at the time of writing, applying those files would be done like so:</p> Bash<pre><code>kubectl apply -f eks-connector.yaml,eks-connector-clusterrole.yaml,eks-connector-console-dashboard-full-access-group.yaml\n</code></pre> <p>Even more info can be found at the eksctl documentation.</p>"},{"location":"parts/eks-connector/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Is there any official alternative?</li> </ul>"},{"location":"parts/eksa-install/","title":"3. EKS-A installation and configuration","text":""},{"location":"parts/eksa-install/#part-3-eks-a-installation-and-configuration","title":"Part 3: EKS-A installation and configuration","text":""},{"location":"parts/eksa-install/#steps","title":"Steps","text":""},{"location":"parts/eksa-install/#1-login-to-eksa-admin","title":"1. Login to eksa-admin","text":"<p>Connect to the eksa-admin server via SSH and define required variables: <code>LC_POOL_ADMIN</code> (<code>${POOL_ADMIN}</code> value), <code>LC_POOL_VIP</code> (<code>${POOL_ADMIN}</code> value), <code>LC_TINK_VIP</code> (<code>${TINK_VIP}</code> value). </p> <p>Pro Tip: The special args and environment setting in command below are just tricks to plumb $POOL_ADMIN and $POOL_VIP into the eksa-admin environment</p> Bash<pre><code>LC_POOL_ADMIN=$POOL_ADMIN LC_POOL_VIP=$POOL_VIP LC_TINK_VIP=$TINK_VIP ssh -o SendEnv=LC_POOL_ADMIN,LC_POOL_VIP,LC_TINK_VIP root@$PUB_ADMIN\n</code></pre> <p>Note: The remaining steps assume you have logged into <code>eksa-admin</code> with the SSH command shown above</p>"},{"location":"parts/eksa-install/#2-install-eks-clis","title":"2. Install EKS CLIs","text":"<p>Install <code>eksctl</code> and the <code>eksctl-anywhere</code> plugin on eksa-admin.</p> Bash<pre><code>curl \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz\" \\\n    --silent --location \\\n    | tar xz -C /tmp\nsudo mv /tmp/eksctl /usr/local/bin/\n</code></pre> Bash<pre><code>export EKSA_RELEASE=\"0.14.3\" OS=\"$(uname -s | tr A-Z a-z)\" RELEASE_NUMBER=30\ncurl \"https://anywhere-assets.eks.amazonaws.com/releases/eks-a/${RELEASE_NUMBER}/artifacts/eks-a/v${EKSA_RELEASE}/${OS}/amd64/eksctl-anywhere-v${EKSA_RELEASE}-${OS}-amd64.tar.gz\" \\\n    --silent --location \\\n    | tar xz ./eksctl-anywhere\nsudo mv ./eksctl-anywhere /usr/local/bin/\n</code></pre>"},{"location":"parts/eksa-install/#3-install-kubectl","title":"3. Install <code>kubectl</code>","text":"Bash<pre><code>snap install kubectl --channel=1.25 --classic\n</code></pre> <p>Version 1.25 matches the version used in the eks-anywhere repository.</p> Alternatively, install via APT. Bash<pre><code>curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\necho \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\napt-get update\napt-get install kubectl\n</code></pre>"},{"location":"parts/eksa-install/#4-install-docker","title":"4. Install <code>Docker</code>","text":"<p>Run the docker install script:</p> Bash<pre><code>curl -fsSL https://get.docker.com -o get-docker.sh \nchmod +x get-docker.sh\n./get-docker.sh\n</code></pre> <p>Alternatively, follow the instructions from https://docs.docker.com/engine/install/ubuntu/.</p>"},{"location":"parts/eksa-install/#5-create-eks-a-cluster-config","title":"5. Create EKS-A Cluster config","text":"Bash<pre><code>export TINKERBELL_HOST_IP=$LC_TINK_VIP\nexport CLUSTER_NAME=\"${USER}-${RANDOM}\"\nexport TINKERBELL_PROVIDER=true\neksctl anywhere generate clusterconfig $CLUSTER_NAME --provider tinkerbell &gt; $CLUSTER_NAME.yaml\n</code></pre> <p>Note: The remaining steps assume you have defined the variables set above</p>"},{"location":"parts/eksa-install/#6-edit-generated-cluster-config-file","title":"6. Edit generated cluster config file","text":""},{"location":"parts/eksa-install/#61-install-yq","title":"6.1. Install yq","text":"Bash<pre><code>snap install yq\n</code></pre>"},{"location":"parts/eksa-install/#62-generate-a-public-ssh-key-and-store-it-in-a-variable-called-ssh_public_key","title":"6.2. Generate a public SSH key and store it in a variable called 'SSH_PUBLIC_KEY'","text":"Bash<pre><code>ssh-keygen -t rsa -f /root/.ssh/id_rsa -q -N \"\"\nexport SSH_PUBLIC_KEY=$(cat /root/.ssh/id_rsa.pub)\n</code></pre>"},{"location":"parts/eksa-install/#63-make-all-necessary-changes-to-the-config-cluster-file","title":"6.3. Make all necessary changes to the config cluster file","text":"<p>By running below <code>yq</code> command you will:</p> <ul> <li>Set control-plane IP <code>host</code> for Cluster resource</li> <li>Set the <code>tinkerbellIP</code> in the <code>TinkerbellDatacenterConfig</code> resource</li> <li>Set the public SSH key created in the previous step for each <code>TinkerbellMachineConfig</code></li> <li>Set the <code>hardwareSelector</code> for each <code>TinkerbellMachineConfig</code> - <code>cp</code> or <code>worker</code></li> <li>Change the <code>templateRef</code> for each <code>TinkerbellMachineConfig</code> section - We will add a <code>TinkerbellTemplateConfig</code> in next step</li> </ul> Bash<pre><code>yq eval -i '\n(select(.kind == \"Cluster\") | .spec.controlPlaneConfiguration.endpoint.host) = env(LC_POOL_VIP) |\n(select(.kind == \"TinkerbellDatacenterConfig\") | .spec.tinkerbellIP) = env(LC_TINK_VIP) |\n(select(.kind == \"TinkerbellMachineConfig\") | (.spec.users[] | select(.name == \"ec2-user\")).sshAuthorizedKeys) = [env(SSH_PUBLIC_KEY)] |\n(select(.kind == \"TinkerbellMachineConfig\" and .metadata.name == env(CLUSTER_NAME) + \"-cp\" ) | .spec.hardwareSelector.type) = \"cp\" |\n(select(.kind == \"TinkerbellMachineConfig\" and .metadata.name == env(CLUSTER_NAME)) | .spec.hardwareSelector.type) = \"worker\" |\n(select(.kind == \"TinkerbellMachineConfig\") | .spec.templateRef.kind) = \"TinkerbellTemplateConfig\" |\n(select(.kind == \"TinkerbellMachineConfig\") | .spec.templateRef.name) = env(CLUSTER_NAME)\n' $CLUSTER_NAME.yaml\n</code></pre>"},{"location":"parts/eksa-install/#64-append-the-following-tinkerbelltemplateconfig-resource-with-the-tinkerbell-settings-to-the-config-cluster-file","title":"6.4. Append the following <code>TinkerbellTemplateConfig</code> resource with the Tinkerbell settings to the config cluster file","text":"YAML<pre><code>cat &lt;&lt; EOF &gt;&gt; $CLUSTER_NAME.yaml\n---\napiVersion: anywhere.eks.amazonaws.com/v1alpha1\nkind: TinkerbellTemplateConfig\nmetadata:\n  name: ${CLUSTER_NAME}\nspec:\n  template:\n    global_timeout: 6000\n    id: \"\"\n    name: ${CLUSTER_NAME}\n    tasks:\n    - actions:\n      - environment:\n          COMPRESSED: \"true\"\n          DEST_DISK: /dev/sda\n          IMG_URL: https://anywhere-assets.eks.amazonaws.com/releases/bundles/29/artifacts/raw/1-25/bottlerocket-v1.25.6-eks-d-1-25-7-eks-a-29-amd64.img.gz\n        image: public.ecr.aws/eks-anywhere/tinkerbell/hub/image2disk:6c0f0d437bde2c836d90b000312c8b25fa1b65e1-eks-a-29\n        name: stream-image\n        timeout: 600\n      - environment:\n          CONTENTS: |\n            # Version is required, it will change as we support\n            # additional settings\n            version = 1\n\n            # \"eno1\" is the interface name\n            # Users may turn on dhcp4 and dhcp6 via boolean\n            [enp1s0f0np0]\n            dhcp4 = true\n            dhcp6 = false\n            # Define this interface as the \"primary\" interface\n            # for the system.  This IP is what kubelet will use\n            # as the node IP.  If none of the interfaces has\n            # \"primary\" set, we choose the first interface in\n            # the file\n            primary = true\n          DEST_DISK: /dev/sda12\n          DEST_PATH: /net.toml\n          DIRMODE: \"0755\"\n          FS_TYPE: ext4\n          GID: \"0\"\n          MODE: \"0644\"\n          UID: \"0\"\n        image: public.ecr.aws/eks-anywhere/tinkerbell/hub/writefile:6c0f0d437bde2c836d90b000312c8b25fa1b65e1-eks-a-29\n        name: write-netplan\n        pid: host\n        timeout: 90\n      - environment:\n          BOOTCONFIG_CONTENTS: |\n            kernel {\n                console = \"ttyS1,115200n8\"\n            }\n          DEST_DISK: /dev/sda12\n          DEST_PATH: /bootconfig.data\n          DIRMODE: \"0700\"\n          FS_TYPE: ext4\n          GID: \"0\"\n          MODE: \"0644\"\n          UID: \"0\"\n        image: public.ecr.aws/eks-anywhere/tinkerbell/hub/writefile:6c0f0d437bde2c836d90b000312c8b25fa1b65e1-eks-a-29\n        name: write-bootconfig\n        pid: host\n        timeout: 90\n      - environment:\n          DEST_DISK: /dev/sda12\n          DEST_PATH: /user-data.toml\n          DIRMODE: \"0700\"\n          FS_TYPE: ext4\n          GID: \"0\"\n          HEGEL_URLS: http://${LC_POOL_ADMIN}:50061,http://${LC_TINK_VIP}:50061\n          MODE: \"0644\"\n          UID: \"0\"\n        image: public.ecr.aws/eks-anywhere/tinkerbell/hub/writefile:6c0f0d437bde2c836d90b000312c8b25fa1b65e1-eks-a-29\n        name: write-user-data\n        pid: host\n        timeout: 90\n      - image: public.ecr.aws/eks-anywhere/tinkerbell/hub/reboot:6c0f0d437bde2c836d90b000312c8b25fa1b65e1-eks-a-29\n        name: reboot-image\n        pid: host\n        timeout: 90\n        volumes:\n        - /worker:/worker\n      name: ${CLUSTER_NAME}\n      volumes:\n        - /dev:/dev\n        - /dev/console:/dev/console\n        - /lib/firmware:/lib/firmware:ro\n      worker: '{{.device_1}}'\n    version: \"0.1\"\nEOF\n</code></pre>"},{"location":"parts/eksa-install/#7-create-an-eks-a-cluster","title":"7. Create an EKS-A Cluster","text":"<p>Double check and be sure <code>$LC_POOL_ADMIN</code> and <code>$CLUSTER_NAME</code> are set correctly before running this (they were passed through SSH or otherwise defined in previous steps). Otherwise manually set them!</p> Bash<pre><code>eksctl anywhere create cluster --filename $CLUSTER_NAME.yaml \\\n--hardware-csv hardware.csv --tinkerbell-bootstrap-ip $LC_POOL_ADMIN\n</code></pre>"},{"location":"parts/eksa-install/#8-reboot-nodes","title":"8. Reboot Nodes","text":"<p>Steps to run locally while <code>eksctl anywhere</code> is creating the cluster</p> <p>When the command above (step 7) indicates that it is <code>Creating new workload cluster</code>, reboot the two nodes. This is to force them attempt to iPXE boot from the tinkerbell stack that <code>eksctl anywhere</code> command creates.</p> <p>Note: that this must be done without interrupting the <code>eksctl anywhere create cluster</code> command</p> <p>Option 1 - You can use this command to automate it, but you'll need to be back on the original host.</p> Bash<pre><code>node_ids=$(metal devices list -o json | jq -r '.[] | select(.hostname | startswith(\"eksa-node\")) | .id')\nfor id in $(echo $node_ids); do\n  metal device reboot -i $id\ndone\n</code></pre> <p>Option 2 - Instead of rebooting the nodes from the host you can force the iPXE boot from your local by accessing each node's SOS console. You can retrieve the uuid and facility code of each node using the metal cli, UI Console or the Equinix Metal's API. By default, any existing ssh key in the project can be used to login.</p> Bash<pre><code>ssh {node-uuid}@sos.{facility-code}.platformequinix.com -i &lt;/path/to/ssh-key&gt;\n</code></pre> <p>Note: After rebooting nodes the <code>eksctl anywhere create cluster</code> command output will hang at <code>Creating new workload cluster</code> for almost 20 min without any further feedback</p>"},{"location":"parts/eksa-install/#9-confirm-success","title":"9. Confirm Success","text":"<p>After 20-30 min you will see the below logs message if the whole process is successful</p> Bash<pre><code>Installing networking on workload cluster\nCreating EKS-A namespace\nInstalling cluster-api providers on workload cluster\nInstalling EKS-A secrets on workload cluster\nInstalling resources on management cluster\nMoving cluster management from bootstrap to workload cluster\nInstalling EKS-A custom components (CRD and controller) on workload cluster\nInstalling EKS-D components on workload cluster\nCreating EKS-A CRDs instances on workload cluster\nInstalling GitOps Toolkit on workload cluster\nGitOps field not specified, bootstrap flux skipped\nWriting cluster config file\nDeleting bootstrap cluster\n:tada: Cluster created!\n--------------------------------------------------------------------------------------\nThe Amazon EKS Anywhere Curated Packages are only available to customers with the\nAmazon EKS Anywhere Enterprise Subscription\n--------------------------------------------------------------------------------------\nEnabling curated packages on the cluster\nInstalling helm chart on cluster    {\"chart\": \"eks-anywhere-packages\", \"version\": \"0.2.30-eks-a-29\"}\n</code></pre>"},{"location":"parts/eksa-install/#10-verify-the-nodes-are-deployed-properly","title":"10. Verify the nodes are deployed properly","text":"<p>To verify the nodes are deployed properly, set the generated cluster kubeconfig file as the default k8s config</p> Bash<pre><code>cp /root/$CLUSTER_NAME/$CLUSTER_NAME-eks-a-cluster.kubeconfig /root/.kube/config\n</code></pre> <p>You can run now below commands to check nodes and pods in cluster</p> Bash<pre><code>kubectl get nodes -o wide\nkubectl get pods -A\n</code></pre>"},{"location":"parts/eksa-install/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Can I scale up Nodes after initial setup?</li> </ul>"},{"location":"parts/infrastructure/","title":"2. Preparing Bare Metal for EKS Anywhere","text":""},{"location":"parts/infrastructure/#part-2-preparing-bare-metal-for-eks-anywhere","title":"Part 2: Preparing Bare Metal for EKS Anywhere","text":""},{"location":"parts/infrastructure/#high-level-diagram","title":"High-Level Diagram","text":""},{"location":"parts/infrastructure/#steps","title":"Steps","text":""},{"location":"parts/infrastructure/#1-create-an-eks-a-admin-machine","title":"1. Create an EKS-A Admin machine","text":"<p>Using the metal-cli:</p> Bash<pre><code>metal device create --plan=m3.small.x86 --metro=da --hostname eksa-admin --operating-system ubuntu_20_04\n</code></pre>"},{"location":"parts/infrastructure/#2-create-a-vlan","title":"2. Create a VLAN","text":"Bash<pre><code>metal vlan create --metro da --description eks-anywhere --vxlan 1000\n</code></pre>"},{"location":"parts/infrastructure/#3-create-a-public-ip-reservation-16-addresses","title":"3. Create a Public IP Reservation (16 addresses)","text":"<p>Create a Public IP Reservation (16 addresses):</p> Bash<pre><code>metal ip request --metro da --type public_ipv4 --quantity 16 --tags eksa\n</code></pre> <p>These variables will be referred to in later steps in executable snippets to refer to specific addresses within the pool. The correct IP reservation is chosen by looking for and expecting a single IP reservation to have the \"eksa\" tag applied.</p> Bash<pre><code>echo \"capture the ID, Network, Gateway, and Netmask using jq\"\nVLAN_ID=$(metal vlan list -o json | jq -r '.virtual_networks | .[] | select(.vxlan == 1000) | .id')\nPOOL_ID=$(metal ip list -o json | jq -r '.[] | select(.tags | contains([\"eksa\"]))? | .id')\nPOOL_NW=$(metal ip list -o json | jq -r '.[] | select(.tags | contains([\"eksa\"]))? | .network')\nPOOL_GW=$(metal ip list -o json | jq -r '.[] | select(.tags | contains([\"eksa\"]))? | .gateway')\nPOOL_NM=$(metal ip list -o json | jq -r '.[] | select(.tags | contains([\"eksa\"]))? | .netmask')\necho \"define POOL_ADMIN - ip assigned to eksa-admin within the VLAN\"\nPOOL_ADMIN=$(python3 -c 'import ipaddress; print(str(ipaddress.IPv4Address(\"'${POOL_GW}'\")+1))')\necho \"define PUB_ADMIN - provisioned IPv4 public address of eks-admin which we can use with ssh\"\nPUB_ADMIN=$(metal devices list  -o json  | jq -r '.[] | select(.hostname==\"eksa-admin\") | .ip_addresses [] | select(contains({\"public\":true,\"address_family\":4})) | .address')\necho \"define PORT_ADMIN - the bond0 port of the eks-admin machine\"\nPORT_ADMIN=$(metal devices list  -o json  | jq -r '.[] | select(.hostname==\"eksa-admin\") | .network_ports [] | select(.name == \"bond0\") | .id')\necho \"define POOL_VIP - the floating IPv4 public address assigned to the current lead kubernetes control plane\"\nPOOL_VIP=$(python3 -c 'import ipaddress; print(str(ipaddress.ip_network(\"'${POOL_NW}'/'${POOL_NM}'\").broadcast_address-1))')\necho \"define TINK_VIP - Tinkerbell Host IP\"\nTINK_VIP=$(python3 -c 'import ipaddress; print(str(ipaddress.ip_network(\"'${POOL_NW}'/'${POOL_NM}'\").broadcast_address-2))')\n</code></pre>"},{"location":"parts/infrastructure/#4-create-a-metal-gateway","title":"4. Create a Metal Gateway","text":"Bash<pre><code>metal gateway create --ip-reservation-id $POOL_ID --virtual-network $VLAN_ID\n</code></pre>"},{"location":"parts/infrastructure/#5-create-tinkerbell-worker-nodes","title":"5. Create Tinkerbell worker nodes","text":"<p>Create two Metal devices <code>eksa-node-001</code> - <code>eksa-node-002</code> with Custom IPXE http://{eks-a-public-address}. These nodes will be provisioned as EKS-A Control Plane OR Worker nodes.</p> Bash<pre><code>for a in {1..2}; do\n  metal device create --plan m3.small.x86 --metro da --hostname eksa-node-00$a \\\n  --ipxe-script-url http://$POOL_ADMIN/ipxe/  --operating-system custom_ipxe\ndone\n</code></pre> <p>Note: that the <code>ipxe-script-url</code> doesn't actually get used in this process, we're just setting it as it's a requirement for using the custom_ipxe operating system type.</p>"},{"location":"parts/infrastructure/#6-network-configuration","title":"6. Network configuration","text":""},{"location":"parts/infrastructure/#61-add-the-vlan-to-the-eks-admin-bond0-port","title":"6.1. Add the vlan to the eks-admin bond0 port","text":"Bash<pre><code>metal port vlan -i $PORT_ADMIN -a $VLAN_ID\n</code></pre> <p>Configure the layer 2 vlan network on eks-admin with this snippet:</p> Bash<pre><code>ssh root@$PUB_ADMIN tee -a /etc/network/interfaces &lt;&lt; EOS\n\nauto bond0.1000\niface bond0.1000 inet static\n  pre-up sleep 5\n  address $POOL_ADMIN\n  netmask $POOL_NM\n  vlan-raw-device bond0\nEOS\n</code></pre> <p>Activate the layer 2 vlan network with this command:</p> Bash<pre><code>ssh root@$PUB_ADMIN systemctl restart networking\n</code></pre>"},{"location":"parts/infrastructure/#62-convert-eksa-node-s-network-ports-to-layer2-unbonded-and-attach-to-the-vlan","title":"6.2. Convert <code>eksa-node-*</code> 's network ports to Layer2-Unbonded and attach to the VLAN.","text":"<p>Note: The <code>eksa-node-*</code> nodes must be fulling provisioned before running this step.</p> Bash<pre><code>node_ids=$(metal devices list -o json | jq -r '.[] | select(.hostname | startswith(\"eksa-node\")) | .id')\n\ni=1\nfor id in $(echo $node_ids); do\n  let i++\n  BOND0_PORT=$(metal devices get -i $id -o json  | jq -r '.network_ports [] | select(.name == \"bond0\") | .id')\n  ETH0_PORT=$(metal devices get -i $id -o json  | jq -r '.network_ports [] | select(.name == \"eth0\") | .id')\n  metal port convert -i $BOND0_PORT --layer2 --bonded=false --force\n  metal port vlan -i $ETH0_PORT -a $VLAN_ID\ndone\n</code></pre>"},{"location":"parts/infrastructure/#7-prepare-hardware-inventory","title":"7. Prepare hardware inventory","text":"<p>Capture the MAC Addresses and create <code>hardware.csv</code> file on <code>eks-admin</code> in <code>/root/</code> (run this on the host with metal cli on it):</p> <p>Create the CSV Header:</p> Bash<pre><code>echo hostname,vendor,mac,ip_address,gateway,netmask,nameservers,disk,labels &gt; hardware.csv\n</code></pre> <p>Use <code>metal</code> and <code>jq</code> to grab HW MAC addresses and add them to the hardware.csv:</p> <p>Note: below command assumes you are launching a single control-plane node. For HA you may need to change the <code>if [ \"$i\" = 1 ];</code> condition to something like <code>if [ \"$i\" &lt;= 3 ];</code> (the number will depend on your setup, usually 3 or 5 cp nodes)</p> Bash<pre><code>node_ids=$(metal devices list -o json | jq -r '.[] | select(.hostname | startswith(\"eksa-node\")) | .id')\n\ni=1\nfor id in $(echo $node_ids); do\n  if [ \"$i\" = 1 ]; then TYPE=cp; else TYPE=worker; fi;\n  NODENAME=\"eks-node-00$i\"\n  let i++\n  MAC=$(metal device get -i $id -o json | jq -r '.network_ports | .[] | select(.name == \"eth0\") | .data.mac')\n  IP=$(python3 -c 'import ipaddress; print(str(ipaddress.IPv4Address(\"'${POOL_GW}'\")+'$i'))')\n  echo \"$NODENAME,Equinix,${MAC},${IP},${POOL_GW},${POOL_NM},8.8.8.8|8.8.4.4,/dev/sda,type=${TYPE}\" &gt;&gt; hardware.csv\ndone\n</code></pre> <p>The BMC fields are omitted because Equinix Metal does not expose the BMC of nodes. EKS Anywhere will skip BMC steps with this configuration.</p> <p>Copy <code>hardware.csv</code> to <code>eksa-admin</code>:</p> Bash<pre><code>scp hardware.csv root@$PUB_ADMIN:/root\n</code></pre> <p>We've now provided the <code>eksa-admin</code> machine with all of the variables and configuration needed in preparation.</p>"},{"location":"parts/infrastructure/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Are there other network configurations available? </li> </ul>"},{"location":"parts/setup/","title":"1. Setup","text":""},{"location":"parts/setup/#part-1-setup","title":"Part 1: Setup","text":"<p>Note:  This workshop is intended to detail the manual installation process and the tools available to run and manage EKS clusters on Equinix Metal. However, a Terraform module is available to automate the entire setup process. If you're more interested in that approach, you can follow the terraform installation instructions and skip parts 1-3 from this workshop.</p> <p>To run this workshop you will need access to an Equinix Metal Account or create a new one following step 1 below.</p> <p>Note: You are responsible for the cost of resources created in your Equinix Metal account while running this workshop.</p>"},{"location":"parts/setup/#pre-requisites","title":"Pre-requisites","text":"<p>The following tools will be needed on your local development environment where you will be running most of the commands in this guide.</p> <ul> <li>A Unix-like environment (Linux, OSX, Windows WSL)</li> <li>jq</li> <li>metal-cli (v0.9.0+)</li> </ul>"},{"location":"parts/setup/#steps","title":"Steps","text":""},{"location":"parts/setup/#1-create-an-equinix-metal-api-key-token","title":"1. Create an Equinix Metal API key (token)","text":"<p>If you have never used Equinix Metal before, please follow the Create an Equinix Metal account 1,2 and 4 steps or you can watch our Getting Started with Equinix Metal video.</p>"},{"location":"parts/setup/#2-install-and-configure-metal-cli","title":"2. Install and Configure Metal CLI","text":"<p>Once you are familiar with the console you may feel more comfortable managing your Equinix Metal resources with the command-line interface tool Metal-cli.</p> <p>Once installed take API Key from step 1 and register it:</p> <p>Execute <code>metal init</code> and provide the requested information:</p> Bash<pre><code>$ metal init\n\nEquinix Metal API Tokens can be obtained through the portal at https://console.equinix.com/.\nSee https://metal.equinix.com/developers/docs/accounts/users/ for more details.\n\nToken (hidden):\nOrganization ID [27703148-e7bf-4a2f-95cf-46e7dddb4bb8]:\nProject ID []:\n\nWriting /Users/You/.config/equinix/metal.yaml\n</code></pre>"},{"location":"parts/setup/#3-verify","title":"3. Verify","text":"Bash<pre><code>$ metal organization get\n\n+--------------------------------------+-------------------------------------+----------------------+\n|                  ID                  |                NAME                 |       CREATED        |\n+--------------------------------------+-------------------------------------+----------------------+\n| 27703148-e7bf-4a2f-95cf-46e7dddb4bb8 | Equinix Terraform                   | 2023-01-01T00:00:00Z |\n+--------------------------------------+-------------------------------------+----------------------+\n</code></pre>"},{"location":"parts/setup/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Is there any automated deployment option?</li> </ul>"}]}